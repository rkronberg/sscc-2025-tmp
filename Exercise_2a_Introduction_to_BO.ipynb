{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHJs6azjC8Il"
   },
   "source": [
    "# Introduction to Active Learning with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzDuxz2EDEw9"
   },
   "source": [
    "Bayesian Optimization (BO) is frequently based on Gaussian Process regression (GPR), a probabilistic regression tool. GPR is used to fit *the statistically most likely function*, given data. A Gaussian process (GP) had two forms: the **prior** (encoding any prior belief about the function) and the **posterior** (see image below, from [Rasmussen & Williams, MIT Press (2006)](http://www.gaussianprocess.org/gpml/)). Given the GP prior and some data points (observations), the Bayes' rule is used to compute the GP posterior. **GP posterior mean** is statistically the most likely function that fits the data. **GP posterior variance** is the measure of confidence on the GP mean, with high confidence where variance vanishes.\n",
    "\n",
    "<img src=\"https://gitlab.com/joalof/bigmax_boss_tutorials/-/raw/319e6ba6dbbf37ac456abc6dac7f4de5ef097312/figures/GPR_example.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBJXVOJFDEw_"
   },
   "source": [
    "In BO, GPR is combined with an acquisition function to sample further data \"on-the-fly\". Acquisition functions sample data points with high information content, which are added to the dataset to iteratively refine the model until convergence. Here, model training and refinement is carried out concurrently with dataset building, which makes this an *active learning* approach. The acquisition functions are typically set up to find the global minimum of the unknown function in as few data points as possible.\n",
    "\n",
    "In this tutorial, we use the free [Bayesian Optimization Structure Search (BOSS)](https://gitlab.com/cest-group/boss) code. See the [documentation website](https://cest-group.gitlab.io/boss/) for tutorials, manual and keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlWKlGryC8In",
    "lines_to_next_cell": 2
   },
   "source": [
    "## 1D example of Bayesian Optimization\n",
    "Let us consider an unknown, non-periodic, 1-dimensional function, and infer its global minimum with BO.\n",
    "First, we load the required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1wK8_INC8Ip",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "\n",
    "from boss.bo.bo_main import BOMain\n",
    "from boss.pp.pp_main import PPMain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbbFkiKjDExE"
   },
   "source": [
    "Next, let's define a simple but non-trivial 1D function by adding a Gaussian to a sine wave. We chose the domain of the variable x to be [0, 7], and because of the sine, the values of f(x) are known to be roughly within [-1; 1]. Since we have an analytic expression for the true function we can go ahead and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "YemkoQ0LC8Ip",
    "lines_to_next_cell": 1,
    "outputId": "dada2d46-7469-4f46-8c9a-fac9d8a30696"
   },
   "outputs": [],
   "source": [
    "# Here we define the functional form\n",
    "def f(X):\n",
    "    x = X[0, 0]\n",
    "    y =  np.sin(x) + 1.5*np.exp(-(x - 4.3)**2)\n",
    "    return y\n",
    "\n",
    "# Here we define the domain and examine the function\n",
    "bounds = np.array([[0., 7.]])\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(bounds[0, 0], bounds[0, 1], 100)\n",
    "y_true = np.array([f(np.atleast_2d(xi)) for xi in x])\n",
    "ax.plot(x, y_true)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('1D function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EA4GyoCC8Ir",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Running BO\n",
    "The first step in applying BO to a problem is to consider problem dimensionality and the optimization domain, which involves setting the upper and lower bound for each dimension. In BOSS software, this can be set by keywords in an input file, as listed below.\n",
    "\n",
    "From our problem above, we pass the sampling function and the bounds as an argument. A single entry in the `kernel` array indicates a 1D problem, with an expected `yrange` of approximately [-1; 1]. We begin BO with 2 initial points and acquire 5 more.\n",
    "\n",
    "Once the BO run is defined, we can execute it using the run function, which produces the the `boss.out` (main output file), `boss.rst` (restart file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbkFz5egC8It"
   },
   "outputs": [],
   "source": [
    "# Here we define the BO run\n",
    "bo = BOMain(\n",
    "    f,\n",
    "    bounds,\n",
    "    yrange=[-1, 1],\n",
    "    kernel='rbf',\n",
    "    initpts=2,\n",
    "    iterpts=5,\n",
    ")\n",
    "\n",
    "# Here we perform BO\n",
    "result = bo.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ9_AjA3C8It"
   },
   "source": [
    "To check the quality of the BO, we set up an run the BO postprocessing (PP) routines. The result is the `postprocessing` folder with graphs and raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wqmj8WqrC8Iu"
   },
   "outputs": [],
   "source": [
    "pp = PPMain(result, pp_models=True, pp_acq_funcs=True, pp_truef_npts=100)\n",
    "# you can add more pp keywords here if needed\n",
    "pp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKy8LL1oC8Iu"
   },
   "source": [
    "## Results and Analysis\n",
    "Let's visualize the quality of the GP regression fit by checking the results in the `postprocessing` folder. This typically contains:\n",
    "* a record of data acquisitions (locations and values), with the global minimum predictions\n",
    "* a set of figures showing the GPR fit at every iteration\n",
    "* a set of figures showing the GPR acquisition function at every iteration\n",
    "* global minimum convergence monitoring\n",
    "* a record of hyperparameter evolution throughout the run.\n",
    "\n",
    "You can tailor the amount of output data with postprocessing `keywords`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuPc6mqXDExM"
   },
   "source": [
    "### 1. Acquisition data check\n",
    "For the acquired points, let's review the **acquisition locations** (in x, lower graph and y, upper graph), alongside the (x,y) tracking of the **global minimum prediction** (full line).\n",
    "The key quality check questions to ask are:\n",
    "\n",
    "* is the sampling distributed across the entire x domain?\n",
    "* are there any errors in y function evaluation (are the y values distributed across the expected domain)?\n",
    "* is the predicted global minimum location (full line) converging?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "CGO61RFkIY7Z",
    "outputId": "229f73ed-0fc5-4507-d0f1-152d2aa29839"
   },
   "outputs": [],
   "source": [
    "# Let's inspect the data collection in BOSS\n",
    "Image(filename = \"postprocessing/acquisition_locations.png\", width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ENsrlfmDExN"
   },
   "source": [
    "### 2. Model refinement check\n",
    "Let us review the **progression of the model fitting** with 1, 3 and 5 BO acquisitions (execute below) against the true function.<br> The key quality check questions to ask are:\n",
    "\n",
    "* Is the model converging?\n",
    "* Has the global minimum been found?\n",
    "* Is the model uncertainty reducing?\n",
    "* Has the entire model been converged to the true function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "0XK_9GJgMyFl",
    "outputId": "de151406-253c-45c2-f3c5-43d1ba1346c0"
   },
   "outputs": [],
   "source": [
    "# Let's inspect model fitting in BOSS\n",
    "\n",
    "# figure size in inches optional\n",
    "rcParams['figure.figsize'] = 18, 14\n",
    "\n",
    "# read images\n",
    "it1 = mpimg.imread('postprocessing/graphs_models/it0000_npts0002.png')\n",
    "it3 = mpimg.imread('postprocessing/graphs_models/it0003_npts0005.png')\n",
    "it5 = mpimg.imread('postprocessing/graphs_models/it0005_npts0007.png')\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].imshow(it1)\n",
    "ax[1].imshow(it3)\n",
    "ax[2].imshow(it5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Txz6oRODExO"
   },
   "source": [
    "### 3. Model convergence check\n",
    "Let's consider at the **rate of convergence** of the global minimum in both x and y (left image), as well as the **GPR hyperparameter convergence** with BO iterations (right image). The key quality check questions to ask are:\n",
    "\n",
    "* is the **rate of convergence** reducing on the logarithmic scale?\n",
    "* has the global minimum been evaluated with a sufficient level of accuracy, given data range?\n",
    "* are the GPR hyperparameters - GP lengthscale and variance - converging?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "yFdJnMV4Vlbw",
    "outputId": "e5dbedb6-9874-4c0b-d165-ddf7230ca040"
   },
   "outputs": [],
   "source": [
    "# Let's inspect model convergence in BOSS\n",
    "\n",
    "# figure size in inches optional\n",
    "rcParams['figure.figsize'] = 18, 14\n",
    "\n",
    "# read images\n",
    "conv = mpimg.imread('postprocessing/convergence_measures.png')\n",
    "param = mpimg.imread('postprocessing/hyperparameters.png')\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1,2, frameon=False)\n",
    "ax[0].imshow(conv)\n",
    "ax[1].imshow(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ax_2keSDExP"
   },
   "source": [
    "# Exercise:\n",
    "Re-define the BOSS run with 20 BO iterations and redo all quality check points. <br> Has the convergence improved? Has the model converged to the true function?\n",
    "\n",
    "If you start the BO run with more initial points, does the model take fewer points to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
